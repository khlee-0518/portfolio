✅AI 상보형 트랜스포머
	CNN(로컬 처리)과 트랜스포머(전역 처리)를 보완적으로 결합하여 시각 정보를 효과적으로 학습

✅AI CNN(로컬 처리)
	작은 필터 (예: 3×3 컨볼루션)를 사용하여 인접 픽셀 간의 로컬 특징 추출
	
✅AI 트랜스포머(전역처리)
1. 영상 및 패치 정보								
	- 입력 영상: 224 × 224 × 3 (RGB)
	- 패치 크기: 16 × 16
	- 패치 수: (224 × 224) / (16 × 16) = 196개
	- 각 패치: 16 × 16 × 3 = 768차원 
		→ `patch_vector ∈ R^(768)
			하나의 이미지 패치(16×16×3=768)를 1차원으로 펼친 벡터

2. patch_embedding = patch_vector × W + b
	(선형 변환)
	W : 768 × D 크기 가중치 행렬(768행 x D열) 
	W ∈ R^(768xD)
	b : D차원 편향 벡터

3. 	X = patch_embedding + positional_embedding
    → 각 패치에 위치 정보 추가

	X ∈ R^{196 × D}
    → 196개의 D차원 벡터가 행으로 쌓인 행렬
		행렬 X는 196행 D열 의 실수 행렬에 속한다

	X ∈	
		|패치 1(D차원)		|
​		|패치 2(D차원)		|
​		|~~				|
		|~~				|
		|패치 196(D차원)	|

4. 정리
	768차원의 patch_vector에
	W∈R 768×D를 곱해
	각 패치를 D차원으로 임베딩한 후,
	이걸 196개 행으로 쌓은 것

5. 구조 요약
	- patch_vector ∈ R^(768)
	- W ∈ R^(768xD)
	- patch_embedding = patch_vector × W ∈ R^(D)
	- 총 196개를 행으로 쌓아 X ∈ ℝ^{196 × D}

✅전체 처리 흐름 요약
	이미지 (224×224×3)
	↓
	패치 분할: 16×16 블록 196개 생성
	↓
	각 패치 → 16×16×3 = 768차원 벡터  (patch_vector)
	↓
	선형 변환: patch_vector × W (768×D 행렬) + b
	↓
	D차원 벡터 생성 (patch_embedding)
	↓
	모든 패치 벡터를 행 단위로 쌓음 → X ∈ R^{196 × D}
	↓
	위치 인코딩 추가: X = patch_embedding + positional_embedding


✅ D차원은 어떻게 결정할까?
D는 **하이퍼파라미터(hyperparameter)**예요.
즉, 모델을 만들기 전에 사람이 다음과 같은 기준으로 정해요:

🔹 1. 모델 크기에 따라
	모델 크기	D (임베딩 차원)
	Small	128, 256, 384
	Base	512, 768
	Large	1024, 1280, 1600 등

	예:	ViT-Base는 𝐷=768
		ViT-Large는 𝐷=1024

🔹 2. 계산 자원 (GPU 성능)
	D가 클수록 → 연산량 ↑, 메모리 사용 ↑, 성능 ↑ (하지만 느리고 무거움)
	작은 GPU에서 학습하려면 D를 작게 설정해야 함